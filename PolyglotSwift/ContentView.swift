//
//  ContentView.swift
//  PolyglotSwift
//
//  Created by Ethan on 2024-12-08.
//

import SwiftUI
import CoreData
import AVFoundation
import Speech
import NaturalLanguage
import Translation

class SubtitleWindowManager: ObservableObject {
    private var subtitleWindow: NSWindow?
    private let configuration: TranslationSession.Configuration?
    private lazy var tagger: NSLinguisticTagger = {
        let schemes = [NSLinguisticTagScheme.tokenType]
        return NSLinguisticTagger(tagSchemes: schemes, options: 0)
    }()
    @Published var lastCompleteSentence: String = ""  // å­˜å‚¨ä¸Šä¸€æ¡å®Œæ•´çš„å¥å­
    @Published var lastTranslation: String = ""       // å­˜å‚¨ä¸Šä¸€æ¡å¥å­çš„ç¿»è¯‘
    @Published var lastTwoSentences: [String] = []  // Add this property
    private var translatedSentences: Set<String> = []  // Add this property
    private var selectedInputLanguage: String
    private var selectedOutputLanguage: String
    
    init(configuration: TranslationSession.Configuration?, inputLanguage: String, outputLanguage: String) {
        self.configuration = configuration
        self.selectedInputLanguage = inputLanguage
        self.selectedOutputLanguage = outputLanguage
    }
    
    @Published var subtitleText: String = "" {
        didSet {
            if !subtitleText.isEmpty {
                // å°†æ–‡æœ¬æŒ‰å•è¯åˆ†ç»„
                let words = subtitleText.split(separator: " ")
                let chunks = words.chunked(into: 18)
                if let lastChunk = chunks.last {
                    // å°†æœ€åä¸€ç»„å•è¯é‡æ–°ç»„åˆæˆå¥å­
                    lastCompleteSentence = lastChunk.joined(separator: " ")
                }
                // ç¿»è¯‘å€’æ•°ç¬¬äºŒè¡Œ
                if let secondLastChunk = chunks.dropLast().last {
                    let sentence = secondLastChunk.joined(separator: " ")
                    lastTwoSentences.append(sentence)
                    // åªæœ‰æœªç¿»è¯‘è¿‡çš„å¥å­æ‰è¿›è¡Œç¿»è¯‘
                    if !translatedSentences.contains(sentence) {
                        translatedSentences.insert(sentence)
                        translateSegment(sentence)
                    }
                }
            }
        }
    }
    
    private func translateSegment(_ text: String) {
        // languageLangMapping 
        let languageLangMapping = [
            "zh-CN": "cmn",
            "en-US": "eng",
            "fr-FR": "fra",
            "de-DE": "deu",
            "it-IT": "ita",
            "ja-JP": "jpn",
            "ko-KR": "kor",
            "pt-PT": "por",
            "ru-RU": "rus",
            "es-ES": "spa",
            "vi-VN": "vie",
        ]

        let sourceLang = languageLangMapping[selectedInputLanguage] ?? "eng"
        let targetLang = languageLangMapping[selectedOutputLanguage] ?? "cmn"

        Task {
            do {
                let translator = LocalRestTranslator()
                let translatedText = try await translator.translate(text,
                    from: sourceLang,
                    to: targetLang
                )
                DispatchQueue.main.async {
                    self.lastTranslation = translatedText
                }
            } catch {
                print("âŒ Translation error: \(error)")
                DispatchQueue.main.async {
                    self.lastTranslation = "Translation failed"
                }
            }
        }
    }
    
    private func updateWindowContent() {
        DispatchQueue.main.async {
            if let hostingView = self.subtitleWindow?.contentView as? NSHostingView<SubtitleView> {
                hostingView.rootView = SubtitleView(
                    subtitleText: .constant(self.subtitleText),
                    configuration: self.configuration,
                    windowManager: self
                )
            }
        }
    }

    func openSubtitleWindow() {
        let screenSize = NSScreen.main?.frame.size ?? CGSize(width: 800, height: 600)
        let windowWidth: CGFloat = screenSize.width * 0.6
        let windowHeight: CGFloat = 180 + 40  // å¢åŠ 40åƒç´ é«˜åº¦
        let windowX = (screenSize.width - windowWidth) / 2
        let windowY = screenSize.height / 4

        let window = NSWindow(
            contentRect: NSRect(x: windowX, y: windowY, width: windowWidth, height: windowHeight),
            styleMask: [.borderless],
            backing: .buffered, defer: false)
        
        window.isOpaque = false
        window.backgroundColor = .clear
        window.isMovableByWindowBackground = true
        window.isReleasedWhenClosed = false
        window.level = .floating
        
        let subtitleView = SubtitleView(
            subtitleText: .constant(subtitleText), 
            configuration: configuration,
            windowManager: self
        )
        window.contentView = NSHostingView(rootView: subtitleView)
        window.makeKeyAndOrderFront(nil)
        
        subtitleWindow = window
    }

    func closeSubtitleWindow() {
        subtitleWindow?.close()
        subtitleWindow = nil
    }
}

class SpeechRecognitionManager: ObservableObject {
    private var speechRecognizer: SFSpeechRecognizer?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()
    
    weak var windowManager: SubtitleWindowManager?
    weak var statusBarManager: StatusBarManager?
    
    func startSpeechRecognition(outputDevice: String, inputLanguage: String, languages: [String: String?]) {
        print("ğŸ¤ Starting speech recognition")
        print("ğŸ¤ Output Device: \(outputDevice)")
        print("ğŸ¤ Input Language: \(inputLanguage)")
        print("ğŸ¤ Languages: \(languages)")
        
        // 1. è®¾ç½®è¯­éŸ³è¯†åˆ«å™¨
        guard let languageCode = languages[inputLanguage] as? String,
              let recognizer = SFSpeechRecognizer(locale: Locale(identifier: languageCode)) else {
            print("âŒ Speech recognition not supported for language: \(inputLanguage)")
            return
        }
        
        self.speechRecognizer = recognizer
        
        // 2. æ£€æŸ¥è¯­éŸ³è¯†åˆ«æ˜¯å¦å¯ç”¨
        if !recognizer.isAvailable {
            print("âŒ Speech recognition is not available")
            return
        }
        
        do {
            // 4. é…ç½®éŸ³é¢‘å¼•æ“
            let inputNode = audioEngine.inputNode
            
            // 5. åˆ›å»ºè¯†åˆ«è¯·æ±‚
            recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
            guard let recognitionRequest = recognitionRequest else {
                print("âŒ Unable to create recognition request")
                return
            }
            recognitionRequest.shouldReportPartialResults = true
            
            // 6. è®¾ç½®è¯†åˆ«ä»»åŠ¡
            recognitionTask = recognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
                if let error = error {
                    print("âŒ Recognition error: \(error.localizedDescription)")
                    self?.stopRecording()
                    return
                }
                
                if let result = result {
                    DispatchQueue.main.async {
                        self?.windowManager?.subtitleText = result.bestTranscription.formattedString
                    }
                }
                
                if result?.isFinal == true {
                    print("ğŸ¤ Recognition segment completed, continuing...")
                    self?.restartRecognition()
                }
            }
            
            // 7. å®‰è£…éŸ³é¢‘ tap æ¥æ•è·éŸ³é¢‘æ•°æ®
            let format = inputNode.outputFormat(forBus: 0)
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: format) { [weak self] buffer, _ in
                // å‘é€éŸ³é¢‘æ•°æ®åˆ°è¯†åˆ«è¯·æ±‚
                self?.recognitionRequest?.append(buffer)
                
                // è®¡ç®—åˆ†è´å€¼
                let channelData = buffer.floatChannelData?[0]
                if let data = channelData {
                    let frames = buffer.frameLength
                    var sum: Float = 0
                    for i in 0..<frames {
                        sum += data[Int(i)] * data[Int(i)]
                    }
                    let rms = sqrt(sum / Float(frames))
                    let db = 20 * log10(rms)
                    
                    // æ›´æ–°çŠ¶æ€æ çš„åˆ†è´å€¼
                    DispatchQueue.main.async {
                        self?.statusBarManager?.currentDB = max(-60, min(db, 0))
                    }
                }
            }
            
            // 8. åŠ¨éŸ³é¢‘å¼•æ“
            audioEngine.prepare()
            try audioEngine.start()
            
            print("âœ… Speech recognition started successfully")
            
        } catch {
            print("âŒ Setup error: \(error.localizedDescription)")
        }
    }
    
    func stopRecording() {
        print("ğŸ›‘ Stopping recording")
        
        // 1. åœæ­¢éŸ³é¢‘å¼•æ“
        if audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        
        // 2. ç»“æŸè¯­éŸ³è¯†åˆ«è¯·æ±‚
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        
        // 3. å–æ¶ˆè¯­éŸ³è¯†åˆ«ä»»åŠ¡
        recognitionTask?.cancel()
        recognitionTask = nil
        
        // 4. æ›´æ–°UIçŠ¶æ€
        DispatchQueue.main.async { [weak self] in
            self?.statusBarManager?.isRecording = false
            self?.statusBarManager?.currentDB = -60  // é‡ç½®åˆ†è´å€¼
            self?.windowManager?.subtitleText = ""   // æ¸…ç©ºå­—å¹•
        }
        
        print("âœ… Recording stopped successfully")
    }
    
    private func restartRecognition() {
        print("ğŸ”„ Starting restartRecognition...")
        
        // ç»“æŸå½“å‰è¯·æ±‚
        print("ğŸ”„ Cleaning up current recognition session...")
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        recognitionTask?.cancel()
        recognitionTask = nil
        
        // åˆ›å»ºæ–°çš„è¯†åˆ«è¯·æ±‚
        print("ğŸ”„ Creating new recognition request...")
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        recognitionRequest?.shouldReportPartialResults = true
        
        // é‡æ–°å¼€å§‹è¯†åˆ«ä»»åŠ¡
        guard let request = recognitionRequest else {
            print("âŒ Failed to create new recognition request")
            return
        }
        
        guard let speechRecognizer = speechRecognizer else {
            print("âŒ Speech recognizer is nil")
            return
        }
        
        print("ğŸ”„ Starting new recognition task...")
        recognitionTask = speechRecognizer.recognitionTask(with: request, resultHandler: { [weak self] result, error in
            if let error = error {
                print("âŒ New recognition task error: \(error.localizedDescription)")
                return
            }
            
            if let result = result {
                print("âœ… New recognition result received: \(result.bestTranscription.formattedString)")
                DispatchQueue.main.async {
                    self?.windowManager?.subtitleText = result.bestTranscription.formattedString
                }
            }
            
            if result?.isFinal == true {
                print("ğŸ”„ New recognition segment completed, restarting again...")
                self?.restartRecognition()
            }
        })
        
        if recognitionTask != nil {
            print("âœ… New recognition task started successfully")
        } else {
            print("âŒ Failed to start new recognition task")
        }
    }
}

class StatusBarManager: ObservableObject {
    private var statusItem: NSStatusItem?
    @Published var isRecording: Bool = false
    @Published var currentDB: Float = 0.0 {
        didSet {
            updateStatusBarIcon()
        }
    }
    
    init() {
        setupStatusBar()
    }
    
    private func setupStatusBar() {
        statusItem = NSStatusBar.system.statusItem(withLength: NSStatusItem.variableLength)
        updateStatusBarIcon()
    }
    
    private func updateStatusBarIcon() {
        if isRecording {
            statusItem?.button?.title = String(format: "%.1f dB", currentDB)
        } else {
            statusItem?.button?.title = "Stopped"
        }
    }
}

struct ContentView: View {
    @Environment(\.managedObjectContext) private var viewContext
    @State private var selectedOutputDevice: String = ""
    @State private var selectedInputLanguage: String = "English"
    @State private var selectedOutputLanguage: String = "ç®€ä½“ä¸­æ–‡"
    @State private var isRunning: Bool = false
    @StateObject private var windowManager = SubtitleWindowManager(
        configuration: nil, 
        inputLanguage: "en-US", 
        outputLanguage: "zh-CN"
    )
    @StateObject private var speechRecognitionManager = SpeechRecognitionManager()
    @StateObject private var statusBarManager = StatusBarManager()

    @FetchRequest(
        sortDescriptors: [NSSortDescriptor(keyPath: \Item.timestamp, ascending: true)],
        animation: .default)
    private var items: FetchedResults<Item>

    private var audioEngine = AVAudioEngine()
    private var speechRecognizer: SFSpeechRecognizer?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?

    @State private var configuration: TranslationSession.Configuration? = nil

    private var outputDevices: [String] {
        AVCaptureDevice.DiscoverySession(
            deviceTypes: [.builtInMicrophone, .externalUnknown],
            mediaType: .audio,
            position: .unspecified
        ).devices.compactMap { $0.localizedName }
    }

    init() {
        if let blackHoleDevice = outputDevices.first(where: { $0.contains("BlackHole") }) {
            _selectedOutputDevice = State(initialValue: blackHoleDevice)
        } else if let firstDevice = outputDevices.first {
            _selectedOutputDevice = State(initialValue: firstDevice)
        }
    }

    private let inputLanguages = [
        "English": "en-US",
        "French": "fr-FR",
        "German": "de-DE",
        "Italian": "it-IT",
        "Japanese": "ja-JP",
        "Korean": "ko-KR",
        "Portuguese": "pt-PT",
        "Russian": "ru-RU",
        "Spanish": "es-ES",
        "Vietnamese": "vi-VN"
    ]

    private let outputLanguages = ["ç®€ä½“ä¸­æ–‡", "English", "French", "German", "Italian", "Japanese", "Korean", "Portuguese", "Russian", "Spanish", "Vietnamese"]

    class AudioDelegate: NSObject, AVCaptureAudioDataOutputSampleBufferDelegate {
        func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
            // Handle audio buffer here
        }
    }
    
    private let audioDelegate = AudioDelegate()

    var body: some View {
        VStack {
            Picker("Output Device", selection: $selectedOutputDevice) {
                ForEach(outputDevices, id: \.self) { device in
                    Text(device).tag(device)
                }
            }
            .pickerStyle(MenuPickerStyle())
            .disabled(isRunning)

            Picker("Input Language", selection: $selectedInputLanguage) {
                ForEach(inputLanguages.keys.sorted(), id: \.self) { language in
                    Text(language).tag(language)
                }
            }
            .pickerStyle(MenuPickerStyle())
            .disabled(isRunning)
            .onChange(of: selectedInputLanguage) { _ in
                updateConfiguration()
            }

            Picker("Output Language", selection: $selectedOutputLanguage) {
                ForEach(outputLanguages, id: \.self) { language in
                    Text(language).tag(language)
                }
            }
            .pickerStyle(MenuPickerStyle())
            .disabled(isRunning)
            .onChange(of: selectedOutputLanguage) { _ in
                updateConfiguration()
            }

            Button(action: {
                if selectedOutputDevice.contains("BlackHole") {
                    isRunning.toggle()
                    
                    if isRunning {
                        // å…ˆè®¾ç½®çŠ¶æ€æ ç®¡ç†å™¨
                        speechRecognitionManager.statusBarManager = statusBarManager
                        speechRecognitionManager.windowManager = windowManager
                        
                        // ç„¶åæ›´æ–°çŠ¶æ€å¹¶å¯åŠ¨
                        statusBarManager.isRecording = true
                        windowManager.openSubtitleWindow()
                        
                        speechRecognitionManager.startSpeechRecognition(
                            outputDevice: selectedOutputDevice,
                            inputLanguage: selectedInputLanguage,
                            languages: inputLanguages
                        )
                    } else {
                        speechRecognitionManager.stopRecording()
                        windowManager.closeSubtitleWindow()
                    }
                } else {
                    print("Error: Please select BlackHole as the output device.")
                }
            }) {
                Text(isRunning ? "Stop" : "Start")
            }
        }
        .frame(width: 250)
        .padding()
    }

    private func updateConfiguration() {
        // var sourceLanguage: Locale.Language?
        // var targetLanguage: Locale.Language?
        let sourceLanguage = Locale.Language(identifier: selectedInputLanguage)
        let targetLanguage = Locale.Language(identifier: selectedOutputLanguage)


        configuration = TranslationSession.Configuration(
            source: sourceLanguage,
                    target: targetLanguage
        )
    }
}

struct SubtitleView: View {
    @Binding var subtitleText: String
    let configuration: TranslationSession.Configuration?
    @ObservedObject var windowManager: SubtitleWindowManager
    
    var body: some View {
        VStack(spacing: 10) {
            // å€’æ•°ç¬¬äºŒè¡Œæ–‡æœ¬
            if let secondLastSentence = windowManager.lastTwoSentences.last {
                Text(secondLastSentence)
                    .foregroundColor(.white)
                    .font(.system(size: 20))
            }
            
            // å€’æ•°ç¬¬äºŒè¡Œçš„ç¿»è¯‘
            Text(windowManager.lastTranslation)
                .foregroundColor(.yellow)
                .font(.system(size: 20))
            
            // å½“å‰è¡Œ
            Text(windowManager.lastCompleteSentence)
                .foregroundColor(.gray)
                .font(.system(size: 18))
            
            // Translatingæç¤º
            Text("Translating...")
                .foregroundColor(.gray)
                .font(.system(size: 16))
                .italic()
        }
        .padding(.vertical, 30)
        .padding(.horizontal)
        .frame(maxWidth: .infinity, minHeight: 160)
        .background(Color.black.opacity(0.7))
        .cornerRadius(10)
        .padding()
    }
}

private let itemFormatter: DateFormatter = {
    let formatter = DateFormatter()
    formatter.dateStyle = .short
    formatter.timeStyle = .medium
    return formatter
}()

#Preview {
    ContentView().environment(\.managedObjectContext, PersistenceController.preview.container.viewContext)
}

func configureAudioSession() -> AVCaptureSession? {
    let captureSession = AVCaptureSession()
    guard let audioDevice = AVCaptureDevice.default(for: .audio) else {
        print("No audio device found.")
        return nil
    }

    do {
        let audioInput = try AVCaptureDeviceInput(device: audioDevice)
        if captureSession.canAddInput(audioInput) {
            captureSession.addInput(audioInput)
        } else {
            print("Cannot add audio input.")
            return nil
        }
    } catch {
        print("Error setting up audio input: \(error.localizedDescription)")
        return nil
    }

    return captureSession
}

extension Array {
    func chunked(into size: Int) -> [[Element]] {
        return stride(from: 0, to: count, by: size).map {
            Array(self[$0 ..< Swift.min($0 + size, count)])
        }
    }
}
